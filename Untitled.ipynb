{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/senti_train.csv', header = None, encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop([1,2,3,4], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns = ['sentiment', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['score'] = df['sentiment'].apply(lambda x: (x-2)/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         -1\n",
       "1         -1\n",
       "2         -1\n",
       "3         -1\n",
       "4         -1\n",
       "5         -1\n",
       "6         -1\n",
       "7         -1\n",
       "8         -1\n",
       "9         -1\n",
       "10        -1\n",
       "11        -1\n",
       "12        -1\n",
       "13        -1\n",
       "14        -1\n",
       "15        -1\n",
       "16        -1\n",
       "17        -1\n",
       "18        -1\n",
       "19        -1\n",
       "20        -1\n",
       "21        -1\n",
       "22        -1\n",
       "23        -1\n",
       "24        -1\n",
       "25        -1\n",
       "26        -1\n",
       "27        -1\n",
       "28        -1\n",
       "29        -1\n",
       "          ..\n",
       "1599970    1\n",
       "1599971    1\n",
       "1599972    1\n",
       "1599973    1\n",
       "1599974    1\n",
       "1599975    1\n",
       "1599976    1\n",
       "1599977    1\n",
       "1599978    1\n",
       "1599979    1\n",
       "1599980    1\n",
       "1599981    1\n",
       "1599982    1\n",
       "1599983    1\n",
       "1599984    1\n",
       "1599985    1\n",
       "1599986    1\n",
       "1599987    1\n",
       "1599988    1\n",
       "1599989    1\n",
       "1599990    1\n",
       "1599991    1\n",
       "1599992    1\n",
       "1599993    1\n",
       "1599994    1\n",
       "1599995    1\n",
       "1599996    1\n",
       "1599997    1\n",
       "1599998    1\n",
       "1599999    1\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = TfidfVectorizer(stop_words='english', max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = t.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NB = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_tfidf = t.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72843749999999996"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB.score(X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('NB.pkl', 'wb') as f:\n",
    "    pickle.dump(NB, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(t, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access= \"AKIAJAARWQ57D4HCBHRQ\"\n",
    "secret = \"7xdG1e4BPcEmX4EfyGIjMiSJjxyKvnSXndWGYNALm\"\n",
    "data = sc.textFile('s3://'+ access+':'+secret+'@han.tweets.bucket/twitter_dump.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o17.partitions.\n: org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: S3 GET failed for '/%2Ftwitter_dump.txt' XML Error Message: <?xml version=\"1.0\" encoding=\"UTF-8\"?><Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message><AWSAccessKeyId>AKIAJAARWQ57D4HCBHRQ</AWSAccessKeyId><StringToSign>GETTue, 15 Mar 2016 21:36:27 GMT/han.tweets.bucket/%2Ftwitter_dump.txt</StringToSign><SignatureProvided>p583ySfQBca5uJi2gJKIcCx3YNA=</SignatureProvided><StringToSignBytes>47 45 54 0a 0a 0a 54 75 65 2c 20 31 35 20 4d 61 72 20 32 30 31 36 20 32 31 3a 33 36 3a 32 37 20 47 4d 54 0a 2f 68 61 6e 2e 74 77 65 65 74 73 2e 62 75 63 6b 65 74 2f 25 32 46 74 77 69 74 74 65 72 5f 64 75 6d 70 2e 74 78 74</StringToSignBytes><RequestId>79C1E4082DC350F5</RequestId><HostId>mi0sE3UgCMvftp9XYLyi+TggCQYbzrPqLye/Eo6oDRu7s8Wi1JVTpq92KQpx0UIUgdQPIZEj5/U=</HostId></Error>\n\tat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(Jets3tFileSystemStore.java:156)\n\tat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveINode(Jets3tFileSystemStore.java:195)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)\n\tat com.sun.proxy.$Proxy13.retrieveINode(Unknown Source)\n\tat org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus(S3FileSystem.java:332)\n\tat org.apache.hadoop.fs.FileSystem.getFileStatus(FileSystem.java:1397)\n\tat org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1066)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1007)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:177)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:64)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.jets3t.service.S3ServiceException: S3 GET failed for '/%2Ftwitter_dump.txt' XML Error Message: <?xml version=\"1.0\" encoding=\"UTF-8\"?><Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message><AWSAccessKeyId>AKIAJAARWQ57D4HCBHRQ</AWSAccessKeyId><StringToSign>GETTue, 15 Mar 2016 21:36:27 GMT/han.tweets.bucket/%2Ftwitter_dump.txt</StringToSign><SignatureProvided>p583ySfQBca5uJi2gJKIcCx3YNA=</SignatureProvided><StringToSignBytes>47 45 54 0a 0a 0a 54 75 65 2c 20 31 35 20 4d 61 72 20 32 30 31 36 20 32 31 3a 33 36 3a 32 37 20 47 4d 54 0a 2f 68 61 6e 2e 74 77 65 65 74 73 2e 62 75 63 6b 65 74 2f 25 32 46 74 77 69 74 74 65 72 5f 64 75 6d 70 2e 74 78 74</StringToSignBytes><RequestId>79C1E4082DC350F5</RequestId><HostId>mi0sE3UgCMvftp9XYLyi+TggCQYbzrPqLye/Eo6oDRu7s8Wi1JVTpq92KQpx0UIUgdQPIZEj5/U=</HostId></Error>\n\tat org.jets3t.service.impl.rest.httpclient.RestS3Service.performRequest(RestS3Service.java:416)\n\tat org.jets3t.service.impl.rest.httpclient.RestS3Service.performRestGet(RestS3Service.java:752)\n\tat org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectImpl(RestS3Service.java:1601)\n\tat org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectImpl(RestS3Service.java:1544)\n\tat org.jets3t.service.S3Service.getObject(S3Service.java:2072)\n\tat org.jets3t.service.S3Service.getObject(S3Service.java:1310)\n\tat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(Jets3tFileSystemStore.java:144)\n\t... 37 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fca93c6aedeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m-> 1315\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1316\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \"\"\"\n\u001b[0;32m   1266\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m         \u001b[0mtotalParts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1268\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \"\"\"\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/han/anaconda2/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    833\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 835\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/han/anaconda2/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    308\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    309\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o17.partitions.\n: org.apache.hadoop.fs.s3.S3Exception: org.jets3t.service.S3ServiceException: S3 GET failed for '/%2Ftwitter_dump.txt' XML Error Message: <?xml version=\"1.0\" encoding=\"UTF-8\"?><Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message><AWSAccessKeyId>AKIAJAARWQ57D4HCBHRQ</AWSAccessKeyId><StringToSign>GETTue, 15 Mar 2016 21:36:27 GMT/han.tweets.bucket/%2Ftwitter_dump.txt</StringToSign><SignatureProvided>p583ySfQBca5uJi2gJKIcCx3YNA=</SignatureProvided><StringToSignBytes>47 45 54 0a 0a 0a 54 75 65 2c 20 31 35 20 4d 61 72 20 32 30 31 36 20 32 31 3a 33 36 3a 32 37 20 47 4d 54 0a 2f 68 61 6e 2e 74 77 65 65 74 73 2e 62 75 63 6b 65 74 2f 25 32 46 74 77 69 74 74 65 72 5f 64 75 6d 70 2e 74 78 74</StringToSignBytes><RequestId>79C1E4082DC350F5</RequestId><HostId>mi0sE3UgCMvftp9XYLyi+TggCQYbzrPqLye/Eo6oDRu7s8Wi1JVTpq92KQpx0UIUgdQPIZEj5/U=</HostId></Error>\n\tat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(Jets3tFileSystemStore.java:156)\n\tat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.retrieveINode(Jets3tFileSystemStore.java:195)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:85)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:62)\n\tat com.sun.proxy.$Proxy13.retrieveINode(Unknown Source)\n\tat org.apache.hadoop.fs.s3.S3FileSystem.getFileStatus(S3FileSystem.java:332)\n\tat org.apache.hadoop.fs.FileSystem.getFileStatus(FileSystem.java:1397)\n\tat org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1066)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1007)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:177)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:237)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:64)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:46)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.jets3t.service.S3ServiceException: S3 GET failed for '/%2Ftwitter_dump.txt' XML Error Message: <?xml version=\"1.0\" encoding=\"UTF-8\"?><Error><Code>SignatureDoesNotMatch</Code><Message>The request signature we calculated does not match the signature you provided. Check your key and signing method.</Message><AWSAccessKeyId>AKIAJAARWQ57D4HCBHRQ</AWSAccessKeyId><StringToSign>GETTue, 15 Mar 2016 21:36:27 GMT/han.tweets.bucket/%2Ftwitter_dump.txt</StringToSign><SignatureProvided>p583ySfQBca5uJi2gJKIcCx3YNA=</SignatureProvided><StringToSignBytes>47 45 54 0a 0a 0a 54 75 65 2c 20 31 35 20 4d 61 72 20 32 30 31 36 20 32 31 3a 33 36 3a 32 37 20 47 4d 54 0a 2f 68 61 6e 2e 74 77 65 65 74 73 2e 62 75 63 6b 65 74 2f 25 32 46 74 77 69 74 74 65 72 5f 64 75 6d 70 2e 74 78 74</StringToSignBytes><RequestId>79C1E4082DC350F5</RequestId><HostId>mi0sE3UgCMvftp9XYLyi+TggCQYbzrPqLye/Eo6oDRu7s8Wi1JVTpq92KQpx0UIUgdQPIZEj5/U=</HostId></Error>\n\tat org.jets3t.service.impl.rest.httpclient.RestS3Service.performRequest(RestS3Service.java:416)\n\tat org.jets3t.service.impl.rest.httpclient.RestS3Service.performRestGet(RestS3Service.java:752)\n\tat org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectImpl(RestS3Service.java:1601)\n\tat org.jets3t.service.impl.rest.httpclient.RestS3Service.getObjectImpl(RestS3Service.java:1544)\n\tat org.jets3t.service.S3Service.getObject(S3Service.java:2072)\n\tat org.jets3t.service.S3Service.getObject(S3Service.java:1310)\n\tat org.apache.hadoop.fs.s3.Jets3tFileSystemStore.get(Jets3tFileSystemStore.java:144)\n\t... 37 more\n"
     ]
    }
   ],
   "source": [
    "data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N_gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer('english')\n",
    "REGEX = u\"[\\U00002712\\U00002714\\U00002716\\U0000271d\\U00002721\\U00002728\\U00002733\\U00002734\\U00002744\\U00002747\\U0000274c\\U0000274e\\U00002753-\\U00002755\\U00002757\\U00002763\\U00002764\\U00002795-\\U00002797\\U000027a1\\U000027b0\\U000027bf\\U00002934\\U00002935\\U00002b05-\\U00002b07\\U00002b1b\\U00002b1c\\U00002b50\\U00002b55\\U00003030\\U0000303d\\U0001f004\\U0001f0cf\\U0001f170\\U0001f171\\U0001f17e\\U0001f17f\\U0001f18e\\U0001f191-\\U0001f19a\\U0001f201\\U0001f202\\U0001f21a\\U0001f22f\\U0001f232-\\U0001f23a\\U0001f250\\U0001f251\\U0001f300-\\U0001f321\\U0001f324-\\U0001f393\\U0001f396\\U0001f397\\U0001f399-\\U0001f39b\\U0001f39e-\\U0001f3f0\\U0001f3f3-\\U0001f3f5\\U0001f3f7-\\U0001f4fd\\U0001f4ff-\\U0001f53d\\U0001f549-\\U0001f54e\\U0001f550-\\U0001f567\\U0001f56f\\U0001f570\\U0001f573-\\U0001f579\\U0001f587\\U0001f58a-\\U0001f58d\\U0001f590\\U0001f595\\U0001f596\\U0001f5a5\\U0001f5a8\\U0001f5b1\\U0001f5b2\\U0001f5bc\\U0001f5c2-\\U0001f5c4\\U0001f5d1-\\U0001f5d3\\U0001f5dc-\\U0001f5de\\U0001f5e1\\U0001f5e3\\U0001f5ef\\U0001f5f3\\U0001f5fa-\\U0001f64f\\U0001f680-\\U0001f6c5\\U0001f6cb-\\U0001f6d0\\U0001f6e0-\\U0001f6e5\\U0001f6e9\\U0001f6eb\\U0001f6ec\\U0001f6f0\\U0001f6f3\\U0001f910-\\U0001f918\\U0001f980-\\U0001f984\\U0001f9c0\\U00003297\\U00003299\\U000000a9\\U000000ae\\U0000203c\\U00002049\\U00002122\\U00002139\\U00002194-\\U00002199\\U000021a9\\U000021aa\\U0000231a\\U0000231b\\U00002328\\U00002388\\U000023cf\\U000023e9-\\U000023f3\\U000023f8-\\U000023fa\\U000024c2\\U000025aa\\U000025ab\\U000025b6\\U000025c0\\U000025fb-\\U000025fe\\U00002600-\\U00002604\\U0000260e\\U00002611\\U00002614\\U00002615\\U00002618\\U0000261d\\U00002620\\U00002622\\U00002623\\U00002626\\U0000262a\\U0000262e\\U0000262f\\U00002638-\\U0000263a\\U00002648-\\U00002653\\U00002660\\U00002663\\U00002665\\U00002666\\U00002668\\U0000267b\\U0000267f\\U00002692-\\U00002694\\U00002696\\U00002697\\U00002699\\U0000269b\\U0000269c\\U000026a0\\U000026a1\\U000026aa\\U000026ab\\U000026b0\\U000026b1\\U000026bd\\U000026be\\U000026c4\\U000026c5\\U000026c8\\U000026ce\\U000026cf\\U000026d1\\U000026d3\\U000026d4\\U000026e9\\U000026ea\\U000026f0-\\U000026f5\\U000026f7-\\U000026fa\\U000026fd\\U00002702\\U00002705\\U00002708-\\U0000270d\\U0000270f]|[#]\\U000020e3|[*]\\U000020e3|[0]\\U000020e3|[1]\\U000020e3|[2]\\U000020e3|[3]\\U000020e3|[4]\\U000020e3|[5]\\U000020e3|[6]\\U000020e3|[7]\\U000020e3|[8]\\U000020e3|[9]\\U000020e3|\\U0001f1e6[\\U0001f1e8-\\U0001f1ec\\U0001f1ee\\U0001f1f1\\U0001f1f2\\U0001f1f4\\U0001f1f6-\\U0001f1fa\\U0001f1fc\\U0001f1fd\\U0001f1ff]|\\U0001f1e7[\\U0001f1e6\\U0001f1e7\\U0001f1e9-\\U0001f1ef\\U0001f1f1-\\U0001f1f4\\U0001f1f6-\\U0001f1f9\\U0001f1fb\\U0001f1fc\\U0001f1fe\\U0001f1ff]|\\U0001f1e8[\\U0001f1e6\\U0001f1e8\\U0001f1e9\\U0001f1eb-\\U0001f1ee\\U0001f1f0-\\U0001f1f5\\U0001f1f7\\U0001f1fa-\\U0001f1ff]|\\U0001f1e9[\\U0001f1ea\\U0001f1ec\\U0001f1ef\\U0001f1f0\\U0001f1f2\\U0001f1f4\\U0001f1ff]|\\U0001f1ea[\\U0001f1e6\\U0001f1e8\\U0001f1ea\\U0001f1ec\\U0001f1ed\\U0001f1f7-\\U0001f1fa]|\\U0001f1eb[\\U0001f1ee-\\U0001f1f0\\U0001f1f2\\U0001f1f4\\U0001f1f7]|\\U0001f1ec[\\U0001f1e6\\U0001f1e7\\U0001f1e9-\\U0001f1ee\\U0001f1f1-\\U0001f1f3\\U0001f1f5-\\U0001f1fa\\U0001f1fc\\U0001f1fe]|\\U0001f1ed[\\U0001f1f0\\U0001f1f2\\U0001f1f3\\U0001f1f7\\U0001f1f9\\U0001f1fa]|\\U0001f1ee[\\U0001f1e8-\\U0001f1ea\\U0001f1f1-\\U0001f1f4\\U0001f1f6-\\U0001f1f9]|\\U0001f1ef[\\U0001f1ea\\U0001f1f2\\U0001f1f4\\U0001f1f5]|\\U0001f1f0[\\U0001f1ea\\U0001f1ec-\\U0001f1ee\\U0001f1f2\\U0001f1f3\\U0001f1f5\\U0001f1f7\\U0001f1fc\\U0001f1fe\\U0001f1ff]|\\U0001f1f1[\\U0001f1e6-\\U0001f1e8\\U0001f1ee\\U0001f1f0\\U0001f1f7-\\U0001f1fb\\U0001f1fe]|\\U0001f1f2[\\U0001f1e6\\U0001f1e8-\\U0001f1ed\\U0001f1f0-\\U0001f1ff]|\\U0001f1f3[\\U0001f1e6\\U0001f1e8\\U0001f1ea-\\U0001f1ec\\U0001f1ee\\U0001f1f1\\U0001f1f4\\U0001f1f5\\U0001f1f7\\U0001f1fa\\U0001f1ff]|\\U0001f1f4\\U0001f1f2|\\U0001f1f5[\\U0001f1e6\\U0001f1ea-\\U0001f1ed\\U0001f1f0-\\U0001f1f3\\U0001f1f7-\\U0001f1f9\\U0001f1fc\\U0001f1fe]|\\U0001f1f6\\U0001f1e6|\\U0001f1f7[\\U0001f1ea\\U0001f1f4\\U0001f1f8\\U0001f1fa\\U0001f1fc]|\\U0001f1f8[\\U0001f1e6-\\U0001f1ea\\U0001f1ec-\\U0001f1f4\\U0001f1f7-\\U0001f1f9\\U0001f1fb\\U0001f1fd-\\U0001f1ff]|\\U0001f1f9[\\U0001f1e6\\U0001f1e8\\U0001f1e9\\U0001f1eb-\\U0001f1ed\\U0001f1ef-\\U0001f1f4\\U0001f1f7\\U0001f1f9\\U0001f1fb\\U0001f1fc\\U0001f1ff]|\\U0001f1fa[\\U0001f1e6\\U0001f1ec\\U0001f1f2\\U0001f1f8\\U0001f1fe\\U0001f1ff]|\\U0001f1fb[\\U0001f1e6\\U0001f1e8\\U0001f1ea\\U0001f1ec\\U0001f1ee\\U0001f1f3\\U0001f1fa]|\\U0001f1fc[\\U0001f1eb\\U0001f1f8]|\\U0001f1fd\\U0001f1f0|\\U0001f1fe[\\U0001f1ea\\U0001f1f9]|\\U0001f1ff[\\U0001f1e6\\U0001f1f2\\U0001f1fc]|[0-9*#]\\ufe0f\\u20e3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tweet_process(tweet):\n",
    "    KEY = 'text'\n",
    "    try:\n",
    "        tw = json.loads(tweet.strip())\n",
    "        if KEY not in tw or tw['lang']!= 'en':\n",
    "            return None\n",
    "        return tw\n",
    "\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "\n",
    "def emoji_preprocess(tweet, predict=False):\n",
    "    # add space before and after space\n",
    "    for emoji in re.findall(REGEX, tweet):\n",
    "        tweet = tweet.replace(emoji, ' ' + emoji + ' ')\n",
    "\n",
    "    # tokenize and remove rt and @ and https://\n",
    "    tweet = re.sub('\\?', '', tweet)\n",
    "    tweet = re.sub('\\.', '', tweet)\n",
    "    tweet = re.sub(',', '', tweet)\n",
    "    tweet = re.sub('!', '', tweet)\n",
    "\n",
    "    tweet_tmp = [ wd.strip(punctuation) for wd in tweet.strip('rt').split() if not wd.startswith('@') and not wd.startswith('http') and not wd.startswith('#') ]\n",
    "\n",
    "    if predict:\n",
    "        tweet_token = ['<s>'] + tweet_tmp\n",
    "    else:\n",
    "        tweet_token = ['<s>'] + tweet_tmp + ['</s>']\n",
    "\n",
    "    return tweet_token\n",
    "\n",
    "\n",
    "def bigrams(tweet):\n",
    "    return [ ((w1,), w2) for w1, w2 in nltk.bigrams(tweet)]\n",
    "\n",
    "\n",
    "def trigrams(tweet):\n",
    "    # generate trigrams from tweets\n",
    "    return [((w1, w2), w3) for w1, w2, w3 in nltk.trigrams(tweet)]\n",
    "\n",
    "\n",
    "def quadgrams(tweet):\n",
    "    #generate n grams\n",
    "    return [((w1, w2, w3), w4) for w1, w2, w3, w4 in nltk.ngrams(tweet, 4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets =  sc.textFile('data/twitter_dump_small.txt')\\\n",
    "        .filter(lambda tw: len(tw)>1)\\\n",
    "        .filter(lambda tw: 'created_at' in tw)\\\n",
    "        .map(tweet_process)\\\n",
    "        .filter(lambda tw: tw != None)\\\n",
    "        .map(lambda tw: tw['text'].lower() )\\\n",
    "        .map(emoji_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = tweets.flatMap(lambda x: x).distinct().collect()\n",
    "tweets.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_dict = tweets.flatMap(bigrams).map(lambda bg: (bg, 1))\\\n",
    ".reduceByKey(lambda cnt1, cnt2: cnt1+cnt2).map(lambda (((w0,), w1), cnt): (w0, w1, cnt) ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_dict = tweets.flatMap(trigrams).map(lambda bg: (bg, 1))\\\n",
    ".reduceByKey(lambda cnt1, cnt2: cnt1+cnt2).map(lambda (((w0, w1), w2), cnt) : (str((w0, w1)), w2, cnt) ).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "quadgram_dict = tweets.flatMap(quadgrams).map(lambda bg: (bg, 1))\\\n",
    ".reduceByKey(lambda cnt1, cnt2: cnt1+cnt2).map(lambda  (((w0, w1, w2), w3), cnt) :  (str((w0, w1, w2)), w3, cnt) ).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = set(zip(*bigram_dict)[0])\n",
    "index.update(set(zip(*trigram_dict)[0]))\n",
    "index.update(set(zip(*quadgram_dict)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_gram_mod = pd.DataFrame( index = index,  columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            NaN\n",
       "1           NaN\n",
       "1-6pm       NaN\n",
       "10          NaN\n",
       "10-530am    NaN\n",
       "1000s       NaN\n",
       "100mph      NaN\n",
       "10am        NaN\n",
       "10pm        NaN\n",
       "10x         NaN\n",
       "11          NaN\n",
       "11am        NaN\n",
       "11pm        NaN\n",
       "12          NaN\n",
       "12-inch     NaN\n",
       "15          NaN\n",
       "16          NaN\n",
       "17          NaN\n",
       "18          NaN\n",
       "1950        NaN\n",
       "1951        NaN\n",
       "1971        NaN\n",
       "1:49        NaN\n",
       "1st         NaN\n",
       "2           NaN\n",
       "2-1         NaN\n",
       "20          NaN\n",
       "2016        NaN\n",
       "22          NaN\n",
       "26          NaN\n",
       "           ... \n",
       "😊           NaN\n",
       "😋           NaN\n",
       "😌           NaN\n",
       "😍           NaN\n",
       "😎           NaN\n",
       "😏           NaN\n",
       "😑           NaN\n",
       "😒           NaN\n",
       "😓           NaN\n",
       "😔           NaN\n",
       "😕           NaN\n",
       "😘           NaN\n",
       "😜           NaN\n",
       "😝           NaN\n",
       "😞           NaN\n",
       "😢           NaN\n",
       "😣           NaN\n",
       "😤           NaN\n",
       "😩           NaN\n",
       "😬           NaN\n",
       "😭           NaN\n",
       "🙃           NaN\n",
       "🙄           NaN\n",
       "🙈           NaN\n",
       "🙌           NaN\n",
       "🙏           NaN\n",
       "🤓           NaN\n",
       "🤔           NaN\n",
       "🤗           NaN\n",
       "🤘           NaN\n",
       "Name: (u'a', u'fun', u'language'), dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_mod.ix[str(key)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_gram_mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-cc0aff8a36ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbigram_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mn_gram_mod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'n_gram_mod' is not defined"
     ]
    }
   ],
   "source": [
    "for (key, w, cnt) in bigram_dict:\n",
    "    n_gram_mod.ix[key][w] = cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (key, w, cnt) in trigram_dict:\n",
    "    n_gram_mod.ix[key][w] = cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (key, w, cnt) in quadgram_dict:\n",
    "    n_gram_mod.ix[key][w] = cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_gram_mod.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_gram_sparse = n_gram_mod.to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_mod = defaultdict(Counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_model((w0, w1, cnt)):\n",
    "    global bigram_mod\n",
    "    bigram_mod[w0][w1] = cnt\n",
    "    \n",
    "    return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_dict = tweets.flatMap(bigrams).map(lambda bg: (bg, 1))\\\n",
    ".reduceByKey(lambda cnt1, cnt2: cnt1+cnt2).map(lambda (((w0,), w1), cnt): (w0, w1, cnt)).map(bigram_model).collect()\n",
    "# .reduceByKey(lambda dict1, dict2: str(type(dict1)) + str(type(dict2))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.Counter, {'fixer': Counter(), 'test': Counter()})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
